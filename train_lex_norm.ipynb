{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_lex_norm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLtM5EJ4o6Za"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore',category=FutureWarning)\n",
        "\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "\n",
        "from collections import defaultdict \n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow import one_hot\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "TEST = 550\n",
        "EPOCHS = 35\n",
        "MAX_LEN = 50\n",
        "TRAIN = 2400\n",
        "BATCH_SIZE = 16\n",
        "HIDDEN_DIM = 100\n",
        "VECTOR_DIM = 100"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyDmszkSuDof"
      },
      "source": [
        "def get_data(path, name):\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "\n",
        "    data = json.load(open(path))\n",
        "\n",
        "    if name=='train':\n",
        "        data = data[:TRAIN]\n",
        "\n",
        "    elif name=='test':\n",
        "        data = data[-TEST:]\n",
        "\n",
        "    for dict in data:\n",
        "\n",
        "        x = dict['input']\n",
        "        y = dict['output']\n",
        "\n",
        "        n = len(x)\n",
        "\n",
        "        for i in range(n):\n",
        "            x[i] = re.sub('@[^ ]+','<username>',x[i])\n",
        "            x[i] = re.sub('http://[^ ]+','<link>',x[i])\n",
        "\n",
        "            y[i] = re.sub('@[^ ]+','<username>',y[i])\n",
        "            y[i] = re.sub('http://[^ ]+','<link>',y[i])\n",
        "\n",
        "        for _ in range(MAX_LEN-n):\n",
        "            x.append(\"\")\n",
        "            y.append(\"\")\n",
        "        \n",
        "        X.append(x)\n",
        "        Y.append(y)\n",
        "\n",
        "    return X, Y"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7Wfmdd6uIT_"
      },
      "source": [
        "def buildDict(data):\n",
        "\n",
        "    wordToNum = defaultdict(int)\n",
        "    num = 1\n",
        "    for sent in data:\n",
        "        for word in sent:\n",
        "            if not wordToNum[word]:\n",
        "                wordToNum[word] = num\n",
        "                num+= 1\n",
        "\n",
        "    return wordToNum"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bllowJTjuL6e"
      },
      "source": [
        "def buildDictInv(wordToNum):\n",
        "\n",
        "    numToWord = defaultdict(str)\n",
        "\n",
        "    for key in wordToNum.keys():\n",
        "        numToWord[wordToNum[key]]=key\n",
        "\n",
        "    return numToWord"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e234Bo_GuPk2"
      },
      "source": [
        "def tokenize(data,wordToNum):\n",
        "\n",
        "    tokenizedData = []\n",
        "\n",
        "    for sent in data:\n",
        "\n",
        "        tokenizedSent = []\n",
        "        for word in sent:\n",
        "            tokenizedSent.append(wordToNum[word])\n",
        "\n",
        "        tokenizedSent=np.array(tokenizedSent,dtype=float)\n",
        "        tokenizedData.append(tokenizedSent)\n",
        "\n",
        "    return np.array(tokenizedData)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6852y15uSvb"
      },
      "source": [
        "def getModel(VOCAB_SIZE):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Input(shape=(MAX_LEN,)))\n",
        "    model.add(Embedding(VOCAB_SIZE, output_dim=VECTOR_DIM, input_length=MAX_LEN, trainable=True))\n",
        "    model.add(LSTM(HIDDEN_DIM, return_sequences = True))\n",
        "    model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
        "    \n",
        "    print(model.summary())\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGMSIGF0uXF1"
      },
      "source": [
        "def defineModel():\n",
        "\n",
        "    raw, normalized = get_data('data/data.json', 'train')\n",
        "\n",
        "    wordToNum = buildDict(raw + normalized)\n",
        "    numToWord = buildDictInv(wordToNum)\n",
        "    vocab_size = len(wordToNum)+1\n",
        "\n",
        "    model       = getModel(vocab_size)\n",
        "\n",
        "    model.save('model.h5')\n",
        "    pkl.dump(wordToNum, open('data/wordToNum.pkl', 'wb'))\n",
        "    pkl.dump(numToWord, open('data/numToWord.pkl', 'wb'))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRFQxCCCuhRo"
      },
      "source": [
        "def train():\n",
        "\n",
        "  model = keras.models.load_model('model.h5')\n",
        "  wordToNum = pkl.load(open('data/wordToNum.pkl', 'rb'))\n",
        "\n",
        "  raw, normalized = get_data('data/data.json', 'train')\n",
        "  rawValid, normalizedValid = get_data('data/data.json', 'test')\n",
        "\n",
        "  raw = tokenize(raw, wordToNum)\n",
        "  normalized = tokenize(normalized, wordToNum)\n",
        "  rawValid = tokenize(rawValid, wordToNum)\n",
        "  normalizedValid = tokenize(normalizedValid, wordToNum)\n",
        "\n",
        "  x = raw\n",
        "  y = normalized\n",
        "  y = y.reshape(y.shape[0], y.shape[1], 1)\n",
        "\n",
        "  xValid = rawValid\n",
        "  yValid = normalizedValid\n",
        "  yValid = yValid.reshape(yValid.shape[0], yValid.shape[1], 1)\n",
        "\n",
        "  model.fit(x, y, validation_data=(xValid, yValid), batch_size = BATCH_SIZE, epochs = EPOCHS)\n",
        "\n",
        "  model.save('model.h5')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Op-jd3V9u08B",
        "outputId": "7a50c3a1-debe-43d8-c96d-ac9fd53fa5c1"
      },
      "source": [
        "defineModel()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 50, 100)           1382600   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 50, 100)           80400     \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 50, 13826)         1396426   \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 50, 13826)         0         \n",
            "=================================================================\n",
            "Total params: 2,859,426\n",
            "Trainable params: 2,859,426\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3KZyWevvu8P",
        "outputId": "f060c5a2-25db-4f7d-a272-0afc322b1ba7"
      },
      "source": [
        "train()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/35\n",
            "150/150 [==============================] - 6s 35ms/step - loss: 4.0259 - sparse_categorical_accuracy: 0.6981 - val_loss: 2.8767 - val_sparse_categorical_accuracy: 0.6873\n",
            "Epoch 2/35\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 2.3421 - sparse_categorical_accuracy: 0.7042 - val_loss: 2.3024 - val_sparse_categorical_accuracy: 0.7001\n",
            "Epoch 3/35\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 2.0048 - sparse_categorical_accuracy: 0.7203 - val_loss: 2.1577 - val_sparse_categorical_accuracy: 0.7087\n",
            "Epoch 4/35\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 1.8763 - sparse_categorical_accuracy: 0.7293 - val_loss: 2.0747 - val_sparse_categorical_accuracy: 0.7248\n",
            "Epoch 5/35\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 1.7748 - sparse_categorical_accuracy: 0.7494 - val_loss: 1.9964 - val_sparse_categorical_accuracy: 0.7416\n",
            "Epoch 6/35\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 1.6705 - sparse_categorical_accuracy: 0.7617 - val_loss: 1.9092 - val_sparse_categorical_accuracy: 0.7535\n",
            "Epoch 7/35\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 1.5649 - sparse_categorical_accuracy: 0.7746 - val_loss: 1.8277 - val_sparse_categorical_accuracy: 0.7760\n",
            "Epoch 8/35\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 1.4542 - sparse_categorical_accuracy: 0.8012 - val_loss: 1.7436 - val_sparse_categorical_accuracy: 0.7980\n",
            "Epoch 9/35\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 1.3465 - sparse_categorical_accuracy: 0.8171 - val_loss: 1.6681 - val_sparse_categorical_accuracy: 0.8147\n",
            "Epoch 10/35\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 1.2468 - sparse_categorical_accuracy: 0.8318 - val_loss: 1.6043 - val_sparse_categorical_accuracy: 0.8265\n",
            "Epoch 11/35\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 1.1538 - sparse_categorical_accuracy: 0.8449 - val_loss: 1.5430 - val_sparse_categorical_accuracy: 0.8389\n",
            "Epoch 12/35\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 1.0683 - sparse_categorical_accuracy: 0.8558 - val_loss: 1.4983 - val_sparse_categorical_accuracy: 0.8465\n",
            "Epoch 13/35\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.9891 - sparse_categorical_accuracy: 0.8663 - val_loss: 1.4532 - val_sparse_categorical_accuracy: 0.8558\n",
            "Epoch 14/35\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.9145 - sparse_categorical_accuracy: 0.8763 - val_loss: 1.4163 - val_sparse_categorical_accuracy: 0.8639\n",
            "Epoch 15/35\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.8445 - sparse_categorical_accuracy: 0.8847 - val_loss: 1.3829 - val_sparse_categorical_accuracy: 0.8718\n",
            "Epoch 16/35\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.7784 - sparse_categorical_accuracy: 0.8929 - val_loss: 1.3564 - val_sparse_categorical_accuracy: 0.8783\n",
            "Epoch 17/35\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.7162 - sparse_categorical_accuracy: 0.8998 - val_loss: 1.3306 - val_sparse_categorical_accuracy: 0.8828\n",
            "Epoch 18/35\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.6581 - sparse_categorical_accuracy: 0.9068 - val_loss: 1.3135 - val_sparse_categorical_accuracy: 0.8875\n",
            "Epoch 19/35\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.6037 - sparse_categorical_accuracy: 0.9136 - val_loss: 1.2975 - val_sparse_categorical_accuracy: 0.8916\n",
            "Epoch 20/35\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.5521 - sparse_categorical_accuracy: 0.9210 - val_loss: 1.2906 - val_sparse_categorical_accuracy: 0.8951\n",
            "Epoch 21/35\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.5037 - sparse_categorical_accuracy: 0.9279 - val_loss: 1.2726 - val_sparse_categorical_accuracy: 0.8971\n",
            "Epoch 22/35\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.4576 - sparse_categorical_accuracy: 0.9362 - val_loss: 1.2630 - val_sparse_categorical_accuracy: 0.8996\n",
            "Epoch 23/35\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.4144 - sparse_categorical_accuracy: 0.9453 - val_loss: 1.2547 - val_sparse_categorical_accuracy: 0.9007\n",
            "Epoch 24/35\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.3737 - sparse_categorical_accuracy: 0.9559 - val_loss: 1.2452 - val_sparse_categorical_accuracy: 0.9022\n",
            "Epoch 25/35\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.3357 - sparse_categorical_accuracy: 0.9659 - val_loss: 1.2400 - val_sparse_categorical_accuracy: 0.9038\n",
            "Epoch 26/35\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.3007 - sparse_categorical_accuracy: 0.9754 - val_loss: 1.2394 - val_sparse_categorical_accuracy: 0.9052\n",
            "Epoch 27/35\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.2683 - sparse_categorical_accuracy: 0.9826 - val_loss: 1.2356 - val_sparse_categorical_accuracy: 0.9062\n",
            "Epoch 28/35\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.2387 - sparse_categorical_accuracy: 0.9880 - val_loss: 1.2321 - val_sparse_categorical_accuracy: 0.9073\n",
            "Epoch 29/35\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.2114 - sparse_categorical_accuracy: 0.9915 - val_loss: 1.2294 - val_sparse_categorical_accuracy: 0.9081\n",
            "Epoch 30/35\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.1866 - sparse_categorical_accuracy: 0.9938 - val_loss: 1.2294 - val_sparse_categorical_accuracy: 0.9085\n",
            "Epoch 31/35\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.1641 - sparse_categorical_accuracy: 0.9957 - val_loss: 1.2292 - val_sparse_categorical_accuracy: 0.9091\n",
            "Epoch 32/35\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.1440 - sparse_categorical_accuracy: 0.9968 - val_loss: 1.2248 - val_sparse_categorical_accuracy: 0.9096\n",
            "Epoch 33/35\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.1260 - sparse_categorical_accuracy: 0.9974 - val_loss: 1.2255 - val_sparse_categorical_accuracy: 0.9098\n",
            "Epoch 34/35\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.1102 - sparse_categorical_accuracy: 0.9981 - val_loss: 1.2239 - val_sparse_categorical_accuracy: 0.9102\n",
            "Epoch 35/35\n",
            "150/150 [==============================] - 5s 33ms/step - loss: 0.0961 - sparse_categorical_accuracy: 0.9985 - val_loss: 1.2231 - val_sparse_categorical_accuracy: 0.9105\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}